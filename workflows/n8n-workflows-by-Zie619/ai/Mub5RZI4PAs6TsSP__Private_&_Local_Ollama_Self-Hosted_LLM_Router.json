{
  "id": "Mub5RZI4PAs6TsSP",
  "meta": {
    "instanceId": "31e69f7f4a77bf465b805824e303232f0227212ae922d12133a0f96ffeab4fef",
    "templateCredsSetupCompleted": true
  },
  "name": "üîêü¶ôü§ñ Private & Local Ollama Self-Hosted LLM Router",
  "tags": [],
  "nodes": [
    {
      "id": "981e858a-cd2b-49cf-9740-a40ac29bba94",
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "position": [
        420,
        860
      ],
      "webhookId": "3804aa1d-2193-4161-84a1-6f5d1059e092",
      "parameters": {
        "options": {}
      },
      "typeVersion": 1.1
    },
    {
      "id": "a164103c-66cb-44da-aae7-177231f517b4",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -160,
        580
      ],
      "parameters": {
        "color": 7,
        "width": 2360,
        "height": 860,
        "content": "# üîêü¶ôü§ñ ÎπÑÍ≥µÍ∞ú Î∞è Î°úÏª¨ Ollama ÏûêÏ≤¥ Ìò∏Ïä§ÌåÖ + ÎèôÏ†Å LLM ÎùºÏö∞ÌÑ∞"
      },
      "typeVersion": 1
    },
    {
      "id": "2ff955e7-c621-4bee-8baf-91769524f781",
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        640,
        1140
      ],
      "parameters": {
        "color": 7,
        "width": 360,
        "height": 260,
        "content": "Ollama ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏"
      },
      "typeVersion": 1
    },
    {
      "id": "40f42923-830d-44a9-a311-c006d91691b7",
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        320,
        760
      ],
      "parameters": {
        "color": 4,
        "width": 280,
        "height": 300,
        "content": "## üëçÏãúÎèÑÌï¥ Î≥¥ÏÑ∏Ïöî!"
      },
      "typeVersion": 1
    },
    {
      "id": "c49f5ff5-92a7-4a2d-81b5-51272e7972b4",
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        740,
        720
      ],
      "parameters": {
        "color": 3,
        "width": 540,
        "height": 380,
        "content": "## ÏÇ¨Ïö©Ïûê ÌîÑÎ°¨ÌîÑÌä∏ Í∏∞Î∞ò Ollama LLM ÎùºÏö∞ÌÑ∞\n\nüí°Ïù¥ ÏóêÏù¥Ï†ÑÌä∏Îäî ÏÇ¨Ïö©ÏûêÏùò ÌîÑÎ°¨ÌîÑÌä∏Ïóê Í∏∞Î∞òÌïòÏó¨ ÎèôÏ†ÅÏúºÎ°ú Îã§Ïùå AI ÏóêÏù¥Ï†ÑÌä∏Î•º ÏúÑÌïú Ollama LLMÏùÑ ÏÑ†ÌÉùÌï©ÎãàÎã§"
      },
      "typeVersion": 1
    },
    {
      "id": "72ad69f4-a24f-4df2-978e-71c5d3a63733",
      "name": "Ollama Dynamic LLM",
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "position": [
        1560,
        1240
      ],
      "parameters": {
        "model": "={{ $('LLM Router').item.json.output.parseJson().llm }}",
        "options": {}
      },
      "credentials": {
        "ollamaApi": {
          "id": "7aPaLgwpfdMWFYm9",
          "name": "Ollama account 127.0.0.1"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "efc2e47a-1d4b-4879-8670-35a34c946bb6",
      "name": "LLM Router",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "position": [
        880,
        860
      ],
      "parameters": {
        "text": "=Choose the most appropriate LLM model for the following user request. Analyze the task requirements carefully and select the model that will provide optimal performance.  Only choose from the provided list.\n\n<user_input>\n{{ $json.chatInput }}\n</user_input>\n",
        "options": {
          "systemMessage": "<role>\nYou are an expert LLM router that classifies user prompts and selects the most appropriate LLM model based on specific task requirements.\n</role>\n\n<purpose>\nYour task is to analyze user inputs, determine the nature of their request, and select the optimal LLM model that will provide the best performance for their specific needs.\n</purpose>\n\n<classification_rules>\nChoose one of the following LLMs based on their capabilities and the user prompt.  You must only select from the provided LLMs:\n\n## Text-Only Models\n- \"qwq\": Specialized in complex reasoning and solving hard problems. Best for: mathematical reasoning, logical puzzles, scientific explanations, and complex problem-solving tasks.\n\n- \"llama3.2\": Multilingual model (3B size) optimized for dialogue, retrieval, and summarization. Best for: conversations in multiple languages, information retrieval, and text summarization.\n\n- \"phi4\": Lightweight model designed for constrained environments. Best for: scenarios requiring low latency, limited computing resources, while maintaining good reasoning capabilities.\n\n## Coding Models\n- \"qwen2.5-coder:14b\": Code-Specific Qwen model, with significant improvements in code generation, code reasoning, and code fixing.\n\n## Vision-Language Models\n- \"granite3.2-vision\": Specialized in document understanding and data extraction. Best for: analyzing charts, tables, diagrams, infographics, and structured visual content.\n\n- \"llama3.2-vision\": General-purpose visual recognition and reasoning. Best for: image description, visual question answering, and general image understanding tasks.\n</classification_rules>\n\n<model_examples>\nExample tasks for each model:\n- qwq: \"Solve this math problem\", \"Explain quantum physics\", \"Debug this logical fallacy\"\n- llama3.2: \"Translate this text to Spanish\", \"Summarize this article\", \"Have a conversation about history\"\n- phi4: \"Generate a quick response\", \"Provide a concise answer\", \"Process this simple request efficiently\"\n- granite3.2-vision: \"Extract data from this chart\", \"Analyze this financial table\", \"Interpret this technical diagram\"\n- llama3.2-vision: \"Describe what's in this image\", \"What can you tell me about this picture?\", \"Answer questions about this photo\"\n</model_examples>\n\n<decision_tree>\n1. Does the prompt include an image?\n   - YES ‚Üí Go to 2\n   - NO ‚Üí Go to 3\n2. Is the image a document, chart, table, or diagram?\n   - YES ‚Üí Use \"granite3.2-vision\"\n   - NO ‚Üí Use \"llama3.2-vision\"\n3. Does the task require complex reasoning or solving difficult problems?\n   - YES ‚Üí Use \"qwq\"\n   - NO ‚Üí Go to 4\n4. Is the task multilingual or requires summarization/retrieval?\n   - YES ‚Üí Use \"llama3.2\"\n   - NO ‚Üí Use \"phi4\" (for efficiency in simple English tasks)\n</decision_tree>\n\n<decision_framework>\nWhen selecting a model, consider:\n1. Task complexity and reasoning requirements\n2. Visual or multimodal components in the request\n3. Language processing needs (summarization, translation, etc.)\n4. Performance constraints (latency, memory limitations)\n5. Required reasoning capabilities\n6. Coding requirements\n</decision_framework>\n\n<examples>\nExample 1:\nUser input: \"Explain quantum computing principles\"\nSelection: \"qwq\"\nReason: \"This request requires deep reasoning and explanation of complex scientific concepts, making QwQ's enhanced reasoning capabilities ideal.\"\n\nExample 2:\nUser input: \"Describe what's in this image of a chart showing quarterly sales\"\nSelection: \"granite3.2-vision\"\nReason: \"This request involves visual document understanding and data extraction from a chart, which is granite-vision's specialty.\"\n\nExample 3:\nUser input: \"Summarize this article about climate change in Spanish\"\nSelection: \"llama3.2\"\nReason: \"This request requires multilingual capabilities and summarization, which are strengths of Llama 3.2.\"\n\nExample 4:\nUser input: \"I need to create a FastAPI endpoint with Python\"\nSelection: \"qwen2.5-coder:14b\"\nReason: \"This request requires code generation, code reasoning, or code fixing.\"\n</examples>\n\n<error_handling>\nIf the user request is unclear or ambiguous, select the model that offers the most general capabilities while noting the uncertainty in your reasoning. If the request appears to contain harmful content or violates ethical guidelines, respond with an appropriate message about being unable to fulfill the request.\n</error_handling>\n\n<output_format>\nRespond with a single JSON object containing:\n{\n  \"llm\": \"the name of the selected LLM model\",\n  \"reason\": \"a brief, specific explanation of why this model is optimal for the task\"\n}\nAvoid any preamble or further explanation.  Remove all ``` or ``json from response.\n</output_format>\n\n\n"
        },
        "promptType": "define",
        "hasOutputParser": true
      },
      "typeVersion": 1.7
    },
    {
      "id": "d8b07c67-b177-496f-ba97-2b886c2b6f1e",
      "name": "AI Agent with Dynamic LLM",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "position": [
        1660,
        860
      ],
      "parameters": {
        "text": "={{ $('When chat message received').item.json.chatInput }}",
        "options": {
          "systemMessage": ""
        },
        "promptType": "define"
      },
      "typeVersion": 1.7
    },
    {
      "id": "3f005c9c-dd92-4970-b4cf-e105ec75840f",
      "name": "Ollama phi4",
      "type": "@n8n/n8n-nodes-langchain.lmChatOllama",
      "position": [
        780,
        1240
      ],
      "parameters": {
        "model": "phi4:latest",
        "options": {
          "format": "json"
        }
      },
      "credentials": {
        "ollamaApi": {
          "id": "7aPaLgwpfdMWFYm9",
          "name": "Ollama account 127.0.0.1"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "47f6c3dd-1bad-458c-ade1-ec26f455a95d",
      "name": "Router Chat Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "position": [
        1160,
        1240
      ],
      "parameters": {},
      "typeVersion": 1.3
    },
    {
      "id": "06b77321-086a-42cf-808a-27d7064403e4",
      "name": "Agent Chat Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "position": [
        1940,
        1240
      ],
      "parameters": {
        "sessionKey": "={{ $('When chat message received').item.json.sessionId }}",
        "sessionIdType": "customKey"
      },
      "typeVersion": 1.3
    },
    {
      "id": "073ae421-5bbf-4ff9-ae8d-1f515f0b8ed7",
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1520,
        720
      ],
      "parameters": {
        "color": 5,
        "width": 540,
        "height": 380,
        "content": "## AI ÏóêÏù¥Ï†ÑÌä∏ ÎèôÏ†Å Î°úÏª¨ Ollama LLM ÏÇ¨Ïö©\n\nüí°Ïù¥ ÏóêÏù¥Ï†ÑÌä∏Îäî Ïù¥Ï†Ñ Router ÏóêÏù¥Ï†ÑÌä∏ ÏÑ†ÌÉùÏóê Í∏∞Î∞òÌïòÏó¨ Ollama LLMÏùÑ ÏÇ¨Ïö©ÌïòÍ≥† ÏÇ¨Ïö©ÏûêÏùò ÌîÑÎ°¨ÌîÑÌä∏Î•º ÎãµÎ≥ÄÌï©ÎãàÎã§."
      },
      "typeVersion": 1
    },
    {
      "id": "2e118ce5-bfa8-4661-99dd-5e72bc7534c6",
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1020,
        1140
      ],
      "parameters": {
        "color": 7,
        "width": 360,
        "height": 260,
        "content": "ÎùºÏö∞ÌÑ∞ Ï±ÑÌåÖ Î©îÎ™®Î¶¨"
      },
      "typeVersion": 1
    },
    {
      "id": "92fff699-0e96-4161-b4dd-bcac682d3dab",
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1420,
        1140
      ],
      "parameters": {
        "color": 7,
        "width": 360,
        "height": 260,
        "content": "Îã§Ïù¥ÎÇòÎØπ Ollama LLM"
      },
      "typeVersion": 1
    },
    {
      "id": "6f8bc049-9440-4863-a8c6-c8cfafde3dda",
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1800,
        1140
      ],
      "parameters": {
        "color": 7,
        "width": 360,
        "height": 260,
        "content": "ÏóêÏù¥Ï†ÑÌä∏ Ï±ÑÌåÖ Î©îÎ™®Î¶¨"
      },
      "typeVersion": 1
    },
    {
      "id": "88e0d3ec-108b-4136-86ae-6714f4e4b63b",
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -380,
        700
      ],
      "parameters": {
        "width": 640,
        "height": 1020,
        "content": "## ÎàÑÍµ¨Î•º ÏúÑÌïú Í≤ÉÏù∏Í∞Ä?\nÏù¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÌÖúÌîåÎ¶øÏùÄ **AI Ïï†Ìò∏Í∞Ä**, **Í∞úÎ∞úÏûê**, Î∞è **ÌîÑÎùºÏù¥Î≤ÑÏãúÎ•º Ï§ëÏãúÌïòÎäî ÏÇ¨Ïö©Ïûê**Î•º ÏúÑÌï¥ ÏÑ§Í≥ÑÎêòÏóàÏúºÎ©∞, Ïô∏Î∂Ä ÏÑúÎπÑÏä§Ïóê Îç∞Ïù¥ÌÑ∞Î•º Î≥¥ÎÇ¥ÏßÄ ÏïäÍ≥† Î°úÏª¨ ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏(LLMs)Ïùò ÌûòÏùÑ ÌôúÏö©ÌïòÍ≥†Ïûê ÌïòÎäî Ïù¥Îì§ÏùÑ ÎåÄÏÉÅÏúºÎ°ú Ìï©ÎãàÎã§. ÌäπÌûà, OllamaÎ•º Î°úÏª¨ÏóêÏÑú Ïã§ÌñâÌïòÎ©∞ Îã§ÏñëÌïú ÌäπÌôîÎêú Î™®Îç∏ Í∞ÑÏùò ÏßÄÎä•Ìòï ÎùºÏö∞ÌåÖÏùÑ ÏõêÌïòÎäî ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÌÅ∞ Í∞ÄÏπòÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.\n\n## Ïù¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞Í∞Ä Ìï¥Í≤∞ÌïòÎäî Î¨∏Ï†úÎäî Î¨¥ÏóáÏù∏Í∞Ä?\nÏó¨Îü¨ Î°úÏª¨ LLMÏùÑ ÏÇ¨Ïö©ÌïòÎ©∞ Í∞Å Î™®Îç∏Ïù¥ Í∞ÄÏßÑ Í∞ïÏ†êÍ≥º Í∏∞Îä•Ïù¥ Îã§Î•º Îïå, ÌäπÏ†ï ÏûëÏóÖÏóê ÎßûÎäî Î™®Îç∏ÏùÑ ÏàòÎèôÏúºÎ°ú ÏÑ†ÌÉùÌïòÎäî Í≤ÉÏù¥ Ïñ¥Î†§Ïö∏ Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞Îäî ÏÇ¨Ïö©Ïûê ÌîÑÎ°¨ÌîÑÌä∏Î•º ÏûêÎèôÏúºÎ°ú Î∂ÑÏÑùÌïòÏó¨ Í∞ÄÏû• Ï†ÅÌï©Ìïú ÌäπÌôîÎêú Ollama Î™®Îç∏Î°ú ÎùºÏö∞ÌåÖÌïòÏó¨, ÏµúÏ†ÅÏùò ÏÑ±Îä•ÏùÑ Î≥¥Ïû•ÌïòÎ©∞ ÏµúÏ¢Ö ÏÇ¨Ïö©ÏûêÍ∞Ä Í∏∞Ïà† ÏßÄÏãùÏùÑ ÏöîÍµ¨Î∞õÏßÄ ÏïäÎèÑÎ°ù Ìï©ÎãàÎã§.\n\n## Ïù¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞Í∞Ä ÌïòÎäî Ïùº\nÏù¥ ÏßÄÎä•Ìòï ÎùºÏö∞ÌÑ∞Îäî:\n- Îì§Ïñ¥Ïò§Îäî ÏÇ¨Ïö©Ïûê ÌîÑÎ°¨ÌîÑÌä∏Î•º Î∂ÑÏÑùÌïòÏó¨ ÏöîÏ≤≠Ïùò ÏÑ±Í≤©ÏùÑ ÌåêÎã®Ìï©ÎãàÎã§\n- ÏûëÏóÖ ÏöîÍµ¨ÏÇ¨Ìï≠Ïóê Îî∞Îùº Î°úÏª¨ Ïª¨Î†âÏÖòÏóêÏÑú ÏµúÏ†ÅÏùò Ollama Î™®Îç∏ÏùÑ ÏûêÎèôÏúºÎ°ú ÏÑ†ÌÉùÌï©ÎãàÎã§\n- Îã§ÏñëÌïú ÏûëÏóÖÏóê Îî∞Îùº ÌäπÌôîÎêú Î™®Îç∏ Í∞ÑÏóê ÏöîÏ≤≠ÏùÑ ÎùºÏö∞ÌåÖÌï©ÎãàÎã§:\n  - ÌÖçÏä§Ìä∏ Ï†ÑÏö© Î™®Îç∏(qwq, llama3.2, phi4)ÏùÑ Îã§ÏñëÌïú Ï∂îÎ°† Î∞è ÎåÄÌôî ÏûëÏóÖÏóê ÏÇ¨Ïö©\n  - ÏΩîÎìú Ï†ÑÏö© Î™®Îç∏(qwen2.5-coder)ÏùÑ ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç ÏßÄÏõêÏóê ÏÇ¨Ïö©\n  - ÎπÑÏ†Ñ Í∏∞Îä• Î™®Îç∏(granite3.2-vision, llama3.2-vision)ÏùÑ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑùÏóê ÏÇ¨Ïö©\n- ÏùºÍ¥ÄÎêú ÏÉÅÌò∏ÏûëÏö©ÏùÑ ÏúÑÌï¥ ÎåÄÌôî Î©îÎ™®Î¶¨Î•º Ïú†ÏßÄÌï©ÎãàÎã§\n- Î™®Îì† Ï≤òÎ¶¨Î•º Î°úÏª¨ÏóêÏÑú ÏàòÌñâÌïòÏó¨ ÏôÑÎ≤ΩÌïú ÌîÑÎùºÏù¥Î≤ÑÏãúÏôÄ Îç∞Ïù¥ÌÑ∞ Î≥¥ÏïàÏùÑ Î≥¥Ïû•Ìï©ÎãàÎã§\n\n## ÏÑ§Ï†ï\n1. [Ollama](https://ollama.ai/)Î•º Î°úÏª¨Ïóê ÏÑ§ÏπòÌïòÍ≥† Ïã§Ìñâ Ï§ëÏù∏ÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§\n2. Ollama CLIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏõåÌÅ¨ÌîåÎ°úÏö∞Ïóê Ïñ∏Í∏âÎêú ÌïÑÏöîÌïú Î™®Îç∏ÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§(Ïòà: `ollama pull phi4`)\n3. n8nÏóêÏÑú Ollama API ÏûêÍ≤© Ï¶ùÎ™ÖÏùÑ Íµ¨ÏÑ±Ìï©ÎãàÎã§(Í∏∞Î≥∏: http://127.0.0.1:11434)\n4. ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º ÌôúÏÑ±ÌôîÌïòÍ≥† Ï±ÑÌåÖ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î•º ÌÜµÌï¥ ÏÉÅÌò∏ÏûëÏö©ÏùÑ ÏãúÏûëÌï©ÎãàÎã§\n\n## Ïù¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º ÏÇ¨Ïö©Ïûê ÎßûÏ∂§ÏúºÎ°ú Ï°∞Ï†ïÌïòÎäî Î∞©Î≤ï\n- ÏûêÏã†Ïùò Ollama Ïª¨Î†âÏÖòÏóê Îî∞Îùº ÎùºÏö∞ÌÑ∞Ïùò Í≤∞Ï†ï ÌîÑÎ†àÏûÑÏõåÌÅ¨ÏóêÏÑú Î™®Îç∏ÏùÑ Ï∂îÍ∞ÄÌïòÍ±∞ÎÇò Ï†úÍ±∞Ìï©ÎãàÎã§\n- LLM ÎùºÏö∞ÌÑ∞Ïùò ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏Î•º ÏàòÏ†ïÌïòÏó¨ Î™®Îç∏ ÏÑ†ÌÉù Í∏∞Ï§ÄÏùò Ïö∞ÏÑ†ÏàúÏúÑÎ•º Ï°∞Ï†ïÌï©ÎãàÎã§\n- Í≤∞Ï†ï Ìä∏Î¶¨ Î°úÏßÅÏùÑ ÏàòÏ†ïÌïòÏó¨ ÌäπÏ†ï Ïö©Î°ÄÏóê Îçî Ï†ÅÌï©ÌïòÍ≤å ÎßåÎì≠ÎãàÎã§\n- ÌäπÌôîÎêú ÏûÖÎ†•ÏùÑ ÏúÑÌïú Ï∂îÍ∞Ä Ï†ÑÏ≤òÎ¶¨ Îã®Í≥ÑÎ•º Ï∂îÍ∞ÄÌï©ÎãàÎã§\n\nÏù¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞Îäî n8nÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îì† Í≤ÉÏùÑ Î°úÏª¨ÏóêÏÑú Ïú†ÏßÄÌïòÎ©¥ÏÑúÎèÑ ÏßÄÎä•Ìòï Î™®Îç∏ ÏÑ†ÌÉù Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌïòÎäî Ï†ïÍµêÌïú AI Ïò§ÏºÄÏä§Ìä∏Î†àÏù¥ÏÖò ÏãúÏä§ÌÖúÏùÑ ÎßåÎìúÎäî Î∞©Î≤ïÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§."
      },
      "typeVersion": 1
    }
  ],
  "active": false,
  "pinData": {},
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "c36ec004-11a3-4b0f-b2fd-f529ae6413a2",
  "connections": {
    "LLM Router": {
      "main": [
        [
          {
            "node": "AI Agent with Dynamic LLM",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Ollama phi4": {
      "ai_languageModel": [
        [
          {
            "node": "LLM Router",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Agent Chat Memory": {
      "ai_memory": [
        [
          {
            "node": "AI Agent with Dynamic LLM",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "Ollama Dynamic LLM": {
      "ai_languageModel": [
        [
          {
            "node": "AI Agent with Dynamic LLM",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Router Chat Memory": {
      "ai_memory": [
        [
          {
            "node": "LLM Router",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "When chat message received": {
      "main": [
        [
          {
            "node": "LLM Router",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  }
}